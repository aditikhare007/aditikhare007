
## Greetings AI Community 👋 About me [Aditi Khare's Profile Summary] 

## Welcome to Aditi Khare's AI Research Collection ##
### Stay Tuned with Latest Advancements in AI ###
If you find the resources useful Please give star to this repository. Happy Learning !!

AWS & Generative AI Research Specialist-Principal Applied AI Product Engineer & Enterprise Architect @ PepsiCo | IIM-A | Quantum AI | Mojo | Author | Applied AI Research Specialist [Portfolio] | LLMOPS | Langchain |
with 7+ Years of Experience in Fortune 50 Product Companies.

[<img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white">](<https://www.linkedin.com/in/aditi-khare-5840977b/>)
[<img src="https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white">](<https://twitter.com/AditiKh32506701/>)
[<img src="https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white">](<aditikhare007@gmail.com>)
[<img src="https://img.shields.io/badge/Medium-12100E?style=for-the-badge&logo=medium&logoColor=white">](<https://medium.com/@aditikhare007>)

## AI Latest Research Papers Collection ### 

## Quantum AI Research Papers Collection ##

2nd Decemeber 2023 -
Hybrid Quantum Neural Network in High-dimensional Data Classification-https://arxiv.org/abs/2312.01024 

## Gen AI Research Papers Collection ##

1. 11th Dec 2023 - **LLM360-Fully Transparent Open-Source LLMs** -https://arxiv.org/pdf/2312.06550.pdf
2. 12th Dec 2023 - **Mathematical Language Models: A Survey**-https://arxiv.org/abs/2312.07622
3. 13th Dec 2023 - **PromptBench: A Library for Evaluation of Large Language Models**-https://arxiv.org/pdf/2312.07910.pdf
4. 1st Dec 2023 - **Mamba: Linear-Time Sequence Modeling with Selective State Spaces**-https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf
5.14th Dec 2023 - **Distributed Representations of Words and Phrases and their Compositionality (Word2vec)**-https://papers.nips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf
6. 11th Dec 2023 - **Beyond Human Data - Scaling Self-Training for Problem-Solving with Language Models**
   Approach for self-training with feedback that can substantially reduce dependence on human-generated data.
   The model-generated data combined with a reward function improves the performance of LLMs on problem-solving tasks-https://arxiv.org/abs/2312.06585

## Nov 2023 ##
1. 9th Nov 2023 - **A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges**-https://arxiv.org/abs/2311.05112
2. **System 2 Attention** - leverages the reasoning and instruction following capabilities of LLMs to decide what to attend to; it regenerates input context to only include relevant portions before attending to the regenerated context to elicit the final response from the model; increases factuality and outperforms standard attention-based LLMs on tasks such as QA and math world problems - https://arxiv.org/abs/2311.11829),
3. **Advancing Long-Context LLMs** - an overview of the methodologies for enhancing Transformer architecture modules that optimize long-context capabilities across all stages from pre-training to inference.-https://arxiv.org/abs/2311.12351
4. **Parallel Speculative Sampling** - approach to reduce inference time of LLMs based on a variant of speculative sampling and parallel decoding; achieves significant speed-ups (up to 30%) by only learning as little as O(d_emb) additional parameters-https://arxiv.org/abs/2311.13581.
5. **Mirasol3B** - a multimodal model for learning across audio, video, and text which decouples the multimodal modeling into separate, focused autoregressive models; the inputs are processed according to the modalities; this approach can handle longer videos compared to other models and it outperforms state-of-the-art approach on video QA, long video QA, and audio-video-text benchmark-https://arxiv.org/abs/2311.05698
6. **GPQA** - proposes a graduate-level Google-proof QA benchmark consisting of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry; the strongest GPT-4 based baseline achieves 39% accuracy; this benchmark offers scalable oversight experiments that can help obtain reliable and truthful information from modern AI systems that surpass human capabilities-https://arxiv.org/abs/2311.12022
7. **Chain-of-Thought Reasoning to Language Agents** - summary of CoT reasoning, foundational mechanics underpinning CoT techniques, and their application to language agent frameworks.-https://arxiv.org/abs/2311.11797
8. **GAIA** - a benchmark for general AI assistants consisting of real-world questions that require a set of fundamental abilities such as reasoning, multimodal handling, web browsing, and generally tool-use proficiency; shows that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins-https://arxiv.org/abs/2311.12983.
9. **Chain-of-Note** - an approach to improve the robustness and reliability of retrieval-augmented language models in facing noisy, irrelevant documents and in handling unknown scenarios; CoN generates sequential reading notes for the retrieved documents, enabling an evaluation of their relevance to the given question and integrating this information to formulate the final answer; CoN significantly outperforms standard retrieval-augmented language models and achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope-https://arxiv.org/abs/2311.09210.
10. **LLMs for Scientific Discovery** - explores the impact of large language models, particularly GPT-4, across various scientific fields including drug discovery, biology, and computational chemistry; assesses GPT-4's understanding of complex scientific concepts, its problem-solving capabilities, and its potential to advance scientific research through expert-driven case assessments and benchmark testing-https://arxiv.org/abs/2311.07361
11.**Fine-Tuning LLMs for Factuality** - fine-tunes language model for factuality without requiring human labeling; it learns from automatically generated factuality preference rankings and targets open-ended generation settings; it significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality-https://arxiv.org/abs/2311.08401.
12.**Contrastive CoT Prompting** - proposes a contrastive chain of thought method to enhance language model reasoning; the approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes; also proposes an automatic method to construct contrastive demonstrations and demonstrates improvements over CoT promptiing-https://arxiv.org/abs/2311.09277.
13.**A Survey on Language Models for Code** - provides an overview of LLMs for code, including a review of 50+ models, 30+ evaluation tasks, and 500 related works-https://arxiv.org/abs/2311.07989v1
14. **JARVIS-1** - an open-world agent that can perceive multimodal input-https://arxiv.org/abs/2311.05997.
15. **Learning to Filter Context for RAG** - proposes a method that improves the quality of the context provided to the generator via two steps: 1) identifying useful context based on lexical and information-theoretic approaches, and 2) training context filtering models that can filter retrieved contexts at inference; outperforms existing approaches on extractive question answering-https://arxiv.org/abs/2311.08377v1.
16. **MART** - proposes an approach for improving LLM safety with multi-round automatic red-teaming; incorporates automatic adversarial prompt writing and safe response generation, which increases red-teaming scalability and the safety of LLMs; violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing-https://arxiv.org/abs/2311.07689.
17.**LLMs can Deceive Users** - explores the use of an autonomous stock trading agent powered by LLMs; finds that the agent acts upon insider tips and hides the reason behind the trading decision; shows that helpful and safe LLMs can strategically deceive users in a realistic situation without direction instructions or training for deception-https://arxiv.org/abs/2311.07590.
18. **Hallucination in LLMs**-A comprehensive survey-https://arxiv.org/abs/2311.05232.
19. **GPT4All** - outlines technical details of the GPT4All model family along with the open-source repository that aims to democratize access to LLMs-https://arxiv.org/abs/2311.04931.
20. **FreshLLMs** - proposes a dynamic QA benchmark-https://arxiv.org/abs/2310.03214

## OCT 2023 ##                                                                                                                         

1. **Spectron** -New approach for spoken language modeling trained end-to-end to directly process spectrograms; it can be fine-tuned to generate high-quality accurate spoken language; the method surpasses existing spoken language models in speaker preservation and semantic coherence-https://arxiv.org/abs/2305.15255),                        |
2. **LLMs Meet New Knowledge** - presents a benchmark to assess LLMs' abilities in knowledge understanding, differentiation, and association; benchmark results show-https://arxiv.org/abs/2310.14820                  3. **Detecting Pretraining Data from LLMs** - explores the problem of pretraining data detection which aims to determine if a black box model was trained on a given text; proposes a detection method named Min-K% Prob as an effective tool for benchmark example contamination detection, privacy auditing of machine unlearning, and copyrighted text detection in LM’s pertaining data-https://arxiv.org/abs/2310.16789               4. **ConvNets Match Vision Transformers** - evaluates a performant ConvNet architecture pretrained on JFT-4B at scale; observes a log-log scaling law between the held out loss and compute budget; after fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers with comparable compute budgets-https://arxiv.org/abs/2310.16764),                          |
5. **Managing AI Risks** - a short paper outlining risks from upcoming and advanced AI systems, including an examination of social harms, malicious uses, and other potential societal issues emerging from the rapid adoption of autonomous AI systems-https://managing-ai-risks.com/managing_ai_risks.pdf
6. **Branch-Solve-Merge Reasoning in LLMs** - an LLM program that consists of branch, solve, and merge modules parameterized with specific prompts to the base LLM; this enables an LLM to plan a decomposition of task into multiple parallel sub-tasks, independently solve them, and fuse solutions to the sub-tasks; improves evaluation correctness and consistency for multiple LLMs-https://arxiv.org/abs/2310.15123               
7. **LLMs for Software Engineering**-A comprehensive survey of LLMs for software engineering, including open research and technical challenges-https://arxiv.org/abs/2310.03533                                       8. **Self-RAG** - presents a new retrieval-augmented framework that enhances an LM’s quality and factuality through retrieval and self-reflection; trains an LM that adaptively retrieves passages on demand, and generates and reflects on the passages and its own generations using special reflection tokens; it significantly outperforms SoTA LLMs-https://arxiv.org/abs/2310.11511                                                9. **Retrieval-Augmentation for Long-form Question Answering** - explores retrieval-augmented language models on long-form question answering; finds that retrieval is an important component but evidence documents should be carefully added to the LLM; finds that attribution error happens more frequently when retrieved documents lack sufficient information/evidence for answering the question-https://arxiv.org/abs/2310.12150   
10. A Study of LLM-Generated Self-Explanations** - assesses an LLM's capability to self-generate feature attribution explanations; self-explanation is useful to improve performance and truthfulness in LLMs; this capability can be used together with chain-of-thought prompting-https://arxiv.org/abs/2310.11207.
11. **OpenAgents** - an open platform for using and hosting language agents in the wild; includes three agents, including a Data Agent for data analysis, a Plugins Agent with 200+ daily API tools, and a Web Agent for autonomous web browsing-https://arxiv.org/abs/2310.10634v1                                                                                              |
12.**LLMs can Learn Rules** - presents a two-stage framework that learns a rule library for reasoning with LLMs; in the first stage-https://arxiv.org/abs/2310.07064.
13.**Meta Chain-of-Thought Prompting** - a generalizable chain-of-thought-https://arxiv.org/abs/2310.06692.
14. **Improving Retrieval-Augmented LMs with Compressors** - presents two approaches to compress retrieved documents into text summaries before pre-pending them in-context: 1) extractive compressor - selects useful sentences from retrieved documents 2) abstractive compressor - generates summaries by synthesizing information from multiple documents; achieves a compression rate of as low as 6% with minimal loss in performance on language modeling tasks and open domain question answering tasks; the proposed training scheme performs selective augmentation which helps to generate empty summaries when retrieved docs are irrelevant or unhelpful for a task-https://arxiv.org/abs/2310.04408.
15. ** Retrieval meets Long Context LLMs** - compares retrieval augmentation and long-context windows for downstream tasks to investigate if the methods can be combined to get the best of both worlds; an LLM with a 4K context window using simple RAG can achieve comparable performance to a fine-tuned LLM with 16K context; retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes; a retrieval-augmented LLaMA2-70B with a 32K context window outperforms GPT-3.5-turbo-16k on seven long context tasks including question answering and query-based summarization-https://arxiv.org/abs/2310.03025.
16. **StreamingLLM** - a framework that enables efficient streaming LLMs with attention sinks, a phenomenon where the KV states of initial tokens will largely recover the performance of window attention; the emergence of the attention sink is due to strong attention scores towards the initial tokens; this approach enables LLMs trained with finite length attention windows to generalize to infinite sequence length without any additional fine-tuning-https://arxiv.org/abs/2309.17453.
17.**The Dawn of LMMs** - a comprehensive analysis of GPT-4V to deepen the understanding of large multimodal models-https://arxiv.org/abs/2309.17421.
18. **Training LLMs with Pause Tokens** - performs training and inference on LLMs with a learnable <pause> token which helps to delay the model's answer generation and attain performance gains on general understanding tasks of Commonsense QA and math word problem-solving; experiments show that this is only beneficial provided that the delay is introduced in both pertaining and downstream fine-tuning-  https://arxiv.org/abs/2310.02226.
19.**Analogical Prompting** - a new prompting approach to automatically guide the reasoning process of LLMs; the approach is different from chain-of-thought in that it doesn’t require labeled exemplars of the reasoning process; the approach is inspired by analogical reasoning and prompts LMs to self-generate relevant exemplars or knowledge in the context-https://arxiv.org/abs/2310.01714.                                                                                                                                                                                                                                                         ##  SEPT 2023 ##                                                                                                                                                                                                                                                                                                                                                                                                                              1. **AlphaMissense** - an AI model classifying missense variants to help pinpoint the cause of diseases; the model is used to develop a catalogue of genetic mutations; it can categorize 89% of all 71 million possible missense variants as either likely pathogenic or likely benign-https://www.science.org/doi/10.1126/science.adg7492                                                                                            2. **Chain-of-Verification reduces Hallucination in LLMs** - develops a method to enable LLMs to "deliberate" on responses to correct mistakes; include the following steps: 1) draft initial response, 2) plan verification questions to fact-check the draft, 3) answer questions independently to avoid bias from other responses, and 4) generate a final verified response- https://arxiv.org/abs/2309.11495                      3. **Contrastive Decoding Improves Reasoning in Large Language Models** - shows that contrastive decoding leads Llama-65B to outperform Llama 2 and other models on commonsense reasoning and reasoning benchmarks-https://arxiv.org/abs/2309.09117                                                                                                                                                                                       4.  **LongLoRA** - an efficient fine-tuning approach to significantly extend the context windows of pre-trained LLMs; implements shift short attention, a substitute that approximates the standard self-attention pattern during training; it has less GPU memory cost and training time compared to full fine-tuning while not compromising accuracy- https://arxiv.org/abs/2309.12307                                                  5. **LLMs for Generating Structured Data** - studies the use of LLMs for generating complex structured data; proposes a structure-aware fine-tuning method, applied to Llama-7B, which significantly outperform other model like GPT-3.5/4 and Vicuna-13B-https://arxiv.org/abs/2309.08963                                                                                                                                                    6. **Textbooks Are All You Need II** - a new 1.3 billion parameter model trained on 30 billion tokens; the dataset consists of "textbook-quality" synthetically generated data; phi-1.5 competes or outperforms other larger models on reasoning tasks suggesting that data quality plays a more important role than previously thought-https://arxiv.org/abs/2309.05463.
7. **The Rise and Potential of LLM Based Agents** - A Comprehensive overview of LLM based agents; covers from how to construct these agents to how to harness them for good-https://arxiv.org/abs/2309.07864                                                                        |
## AUG 2023 ##

1. **Open Problem and Limitation of RLHF** - provides an overview of open problems and the limitations of RLHF- https://arxiv.org/abs/2307.15217                                                                       2. **Skeleton-of-Thought** - proposes a prompting strategy that firsts generate an answer skeleton and then performs parallel API calls to generate the content of each skeleton point; reports quality improvements in addition to speed-up of up to 2.39x-https://arxiv.org/abs/2307.153373.
3.**MetaGPT** - a framework involving LLM-based multi-agents that encodes human standardized operating procedures (SOPs) to extend complex problem-solving capabilities that mimic efficient human workflows; this enables MetaGPT to perform multifaceted software development, code generation tasks, and even data analysis using tools like AutoGPT and LangChain-https://arxiv.org/abs/2308.00352v2                                  4. **OpenFlamingo** - Introduces a family of autoregressive vision-language models ranging from 3B to 9B parameters; the technical report describes the models, training data, and evaluation suite-https://arxiv.org/abs/2308.01390                  - 

## JULY 2023 ## 

1. Universal Adversarial LLM Attacks** - finds universal and transferable adversarial attacks that cause aligned models like ChatGPT and Bard to generate objectionable behaviors; the approach automatically produces adversarial suffixes using greedy and gradient search-https://arxiv.org/abs/2307.15043.
2. **A Survey on Evaluation of LLMs** - a comprehensive overview of evaluation methods for LLMs focusing on what to evaluate, where to evaluate, and how to evaluate-https://arxiv.org/abs/2307.03109                  3. **How Language Models Use Long Contexts** - finds that LM performance is often highest when relevant information occurs at the beginning or end of the input context; performance degrades when relevant information is provided in the middle of a long context-https://arxiv.org/abs/2307.03172.
4. **LLMs as Effective Text Rankers** - Proposes a prompting technique that enables open-source LLMs to perform state-of-the-art text ranking on standard benchmarks- https://arxiv.org/abs/2306.17563                 5. **Multimodal Generation with Frozen LLMs** - introduces an approach that effectively maps images to the token space of LLMs; enables models like PaLM and GPT-4 to tackle visual tasks without parameter updates; enables multimodal tasks and uses in-context learning to tackle various visual tasks-https://arxiv.org/abs/2306.17842.
6. **CodeGen2.5** - releases a new code LLM trained on 1.5T tokens; the 7B model is on par with >15B code-generation models and it’s optimized for fast sampling-https://arxiv.org/abs/2305.02309.                     
7. **InterCode** - introduces a framework of interactive coding as a reinforcement learning environment; this is different from the typical coding benchmarks that consider a static sequence-to-sequence process.    - https://arxiv.org/abs/2306.14898.

## JUNE 2023 ## 

1. **LeanDojo** - an open-source Lean playground consisting of toolkits, data, models, and benchmarks for theorem proving; also develops ReProver, a retrieval augmented LLM-based prover for theorem solving using premises from a vast math library-https://arxiv.org/abs/2306.15626
2. **Extending Context Window of LLMs**-Extends the context window of LLMs like LLaMA to up to 32K with minimal fine-tuning (within 1000 steps); previous methods for extending the context window are inefficient but this approach attains good performance on several tasks while being more efficient and cost-effective-https://arxiv.org/abs/2306.15595.
3. **Computer Vision Through the Lens of Natural Language** - proposes a modular approach for solving computer vision problems by leveraging LLMs; the LLM is used to reason over outputs from independent and descriptive modules that provide extensive information about an image-https://arxiv.org/abs/2306.16410.
4. **Understanding Theory-of-Mind in LLMs with LLMs** - a framework for procedurally generating evaluations with LLMs; proposes a benchmark to study the social reasoning capabilities of LLMs with LLMs.   https://arxiv.org/abs/2306.15448
5. **Evaluations with No Labels** - A Framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on input text; can be used to monitor LLM behavior on datasets streamed during live model deployment-https://arxiv.org/abs/2306.13651v1), [Tweet
6. **Long-range Language Modeling with Self-Retrieval** - an architecture and training procedure for jointly training a retrieval-augmented language model from scratch for long-range language modeling tasks.        - https://arxiv.org/abs/2306.13421.
7. **Scaling MLPs: A Tale of Inductive Bias** - Shows that the performance of MLPs improves with scale and highlights that lack of inductive bias can be compensated- https://arxiv.org/abs/2306.13575
8. **Textbooks Are All You Need** - introduces a new 1.3B parameter LLM called phi-1; it’s significantly smaller in size and trained for 4 days using a selection of textbook-quality data and synthetic textbooks and exercises with GPT-3.5; achieves promising results on the HumanEval benchmark-https://arxiv.org/abs/2306.11644
9. **RoboCat** - New Foundation agent that can operate different robotic arms and can solve tasks from as few as 100 demonstrations; the self-improving AI agent can self-generate new training data to improve its technique and get more efficient at adapting to new tasks-https://arxiv.org/abs/2306.11706.       |
10. **ClinicalGPT** - Language model optimized through extensive and diverse medical data, including medical records, domain-specific knowledge, and multi-round dialogue consultations.                   https://arxiv.org/abs/2306.09968s        |
11. **An Overview of Catastrophic AI Risks** - provides an overview of the main sources of catastrophic AI risks; the goal is to foster more understanding of these risks and ensure AI systems are developed in a safe manner-https://arxiv.org/abs/2306.12001v1.
12. **AudioPaLM** - fuses text-based and speech-based LMs, PaLM-2 and AudioLM, into a multimodal architecture that supports speech understanding and generation; outperforms existing systems for speech translation tasks with zero-shot speech-to-text translation capabilities-https://arxiv.org/abs/2306.12925v1

## MAY 2023 ## 

1. **Gorilla** - a finetuned LLaMA-based model that surpasses GPT-4 on writing API calls. This capability can help identify the right API, boosting the ability of LLMs to interact with external tools to complete specific tasks-https://arxiv.org/abs/2305.15334.
2. **The False Promise of Imitating Proprietary LLMs** - provides a critical analysis of models that are finetuned on the outputs of a stronger model; argues that model imitation is a false premise and that the higher leverage action to improve open source models is to develop better base models-https://arxiv.org/abs/2305.15717
3. **InstructBLIP** - explores visual-language instruction tuning based on the pre-trained BLIP-2 models; achieves state-of-the-art zero-shot performance on 13 held-out datasets, outperforming BLIP-2 and Flamingo.  https://arxiv.org/abs/2305.06500                                  
4. **Active Retrieval Augmented LLMs** - introduces FLARE, retrieval augmented generation to improve the reliability of LLMs; FLARE actively decides when and what to retrieve across the course of the generation; demonstrates superior or competitive performance on long-form knowledge-intensive generation tasks-https://arxiv.org/abs/2305.06983.
5. **FrugalGPT** - presents strategies to reduce the inference cost associated with using LLMs while improving performance-https://arxiv.org/abs/2305.05176.                                                           6. **StarCoder** - an open-access 15.5B parameter LLM with 8K context length and is trained on large amounts of code spanning 80+ programming languages-https://arxiv.org/abs/2305.06161.                              7. **MultiModal-GPT** - a vision and language model for multi-round dialogue with humans; the model is fine-tuned from OpenFlamingo, with LoRA added in the cross-attention and self-attention parts of the language model-https://arxiv.org/abs/2305.04790
8. **A Cookbook of Self-Supervised Learning** - provides an overview of fundamental techniques and key concepts in SSL; it also introduces practical considerations for implementing SSL methods successfully.         https://arxiv.org/abs/2304.12210                                        
9. **Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond** - a comprehensive and practical guide for practitioners working with LLMs; discusses many use cases with practical applications and limitations of LLMs in real-world scenarios-https://arxiv.org/abs/2304.13712
10. **AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head** - connects ChatGPT with audio foundational models to handle challenging audio tasks and a modality transformation interface to enable spoken dialogue-https://arxiv.org/abs/2304.12995.
11. **DataComp: In search of the next generation of multimodal datasets** - releases a new multimodal dataset benchmark containing 12.8B image-text pairs-https://arxiv.org/abs/2304.14108.                            12. **ChatGPT for Information Extraction** - provides a deeper assessment of ChatGPT's performance on the important information extraction task-https://arxiv.org/abs/2304.11633                                                                                                              |
## MARCH 2023 ##

1. **GPT-4 Technical Report** - GPT-4 - a large multimodal model with broader general knowledge and problem-solving abilities.                                                                                                                                       | [Paper](https://arxiv.org/abs/2303.08774v2), [Tweet](https://twitter.com/dair_ai/status/1637456913993433089?s=20)                                                |
| 2) **LERF: Language Embedded Radiance Fields** - a method for grounding language embeddings from models like CLIP into NeRF; this enables open-ended language queries in 3D.                                                                                         | [Paper](https://arxiv.org/abs/2303.09553), [Tweet](https://twitter.com/dair_ai/status/1637456915658686465?s=20)                                                  |
| 3) **An Overview on Language Models: Recent Developments and Outlook** - an overview of language models covering recent developments and future directions. It also covers topics like linguistic units, structures, training methods, evaluation, and applications. | [Paper](https://arxiv.org/abs/2303.05759), [Tweet](https://twitter.com/omarsar0/status/1635273656858460162?s=20)                                                 |
| 4) **Eliciting Latent Predictions from Transformers with the Tuned Lens** - a method for transformer interpretability that can trace a language model predictions as it develops layer by layer.                                                                     | [Paper](https://arxiv.org/abs/2303.08112), [Tweet](https://twitter.com/dair_ai/status/1637456919819440130?s=20)                                                  |
| 5) **Meet in the Middle: A New Pre-training Paradigm** - a new pre-training paradigm using techniques that jointly improve training data efficiency and capabilities of LMs in the infilling task; performance improvement is shown in code generation tasks.        | [Paper](https://arxiv.org/abs/2303.07295) , [Tweet](https://twitter.com/dair_ai/status/1637456922004561920?s=20)                                                 |
| 6) **Resurrecting Recurrent Neural Networks for Long Sequences** - demonstrates that careful design of deep RNNs using standard signal propagation arguments can recover the performance of deep state-space models on long-range reasoning tasks.                   | [Paper](https://arxiv.org/abs/2303.06349) , [Tweet](https://twitter.com/dair_ai/status/1637456923795521537?s=20)                                                 |
| 7) **UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation** - a new approach to tune a lightweight and versatile retriever to automatically retrieve prompts to improve zero-shot performance and help mitigate hallucinations.                     | [Paper](https://arxiv.org/abs/2303.08518), [Tweet](https://twitter.com/dair_ai/status/1637456925779456000?s=20)                                                  |
| 8) **Patches Are All You Need?** - proposes ConvMixer, a parameter-efficient fully-convolutional model which replaces self-attention and MLP layers in ViTs with less-expressive depthwise and pointwise convolutional layers.                                       | [Paper](https://openreview.net/forum?id=rAnB7JSMXL), [Tweet](https://twitter.com/dair_ai/status/1637456927784329218?s=20)                                        |
| 9) **NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes** - a compact and flexible architecture that enables easy 3D surface reconstruction from any NeRF-driven approach; distills NeRFs into geometrically-accurate 3D meshes.   | [Paper](https://arxiv.org/abs/2303.09431), [Tweet](https://twitter.com/dair_ai/status/1637456929705295873?s=20)                                                  |
| 10) **High-throughput Generative Inference of Large Language Models with a Single GPU** - a high-throughput generation engine for running LLMs with limited GPU memory.                                                                                              | [Paper](https://arxiv.org/abs/2303.06865), [Code](https://github.com/FMInference/FlexGen) , [Tweet](https://twitter.com/dair_ai/status/1637456931429183489?s=20) |

---

## 6th March - 12th March 2023 ##

| **Paper**                                                                                                                                                                                                                                                                             | **Links**                                                                                                                                                                                                         |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1) **PaLM-E: An Embodied Multimodal Language Model** - incorporates real-world continuous sensor modalities resulting in an embodied LM that performs tasks such as robotic manipulation planning, visual QA, and other embodied reasoning tasks.                                     | [Paper](https://arxiv.org/abs/2303.03378), [Demo](https://palm-e.github.io/) , [Tweet](https://twitter.com/dair_ai/status/1634919222420836358?s=20)                                                               |
| 2) **Prismer: A Vision-Language Model with An Ensemble of Experts** - a parameter-efficient vision-language model powered by an ensemble of domain experts; it efficiently pools expert knowledge from different domains and adapts it to various vision-language reasoning tasks.    | [Paper](https://arxiv.org/abs/2303.02506), [GitHub](https://github.com/NVlabs/Prismer), [Project](https://shikun.io/projects/prismer) , [Tweet](https://twitter.com/dair_ai/status/1634919224505257985?s=20)      |
| 3) **Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models** - it connects ChatGPT and different visual foundation models to enable users to interact with ChatGPT beyond language format.                                                                       | [Paper](https://arxiv.org/abs/2303.04671), [GitHub](https://github.com/microsoft/visual-chatgpt) [Tweet](https://twitter.com/dair_ai/status/1634919226396794882?s=20)                                             |
| 4) **A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT** - an overview of generative AI - from GAN to ChatGPT.                                                                                                                    | [Paper](https://arxiv.org/abs/2303.04226), [Tweet](https://twitter.com/dair_ai/status/1634919228339003393?s=20)                                                                                                   |
| 5) **Larger language models do in-context learning differently** - shows that with scale, LLMs can override semantic priors when presented with enough flipped labels; these models can also perform well when replacing targets with semantically-unrelated targets.                 | [Paper](https://arxiv.org/abs/2303.03846) , [Tweet](https://twitter.com/dair_ai/status/1634919230461345797?s=20)                                                                                                  |
| 6) **Foundation Models for Decision Making: Problems, Methods, and Opportunities** - provides an overview of foundation models for decision making, including tools, methods, and new research directions.                                                                            | [Project](https://arxiv.org/abs/2303.04129) , [Tweet](https://twitter.com/dair_ai/status/1634919232650760192?s=20)                                                                                                |
| 7) **Hyena Hierarchy: Towards Larger Convolutional Language Models** - a subquadratic drop-in replacement for attention; it interleaves implicit long convolutions and data-controlled gating and can learn on sequences 10x longer and up to 100x faster than optimized attention.   | [Paper](https://arxiv.org/abs/2302.10866), [Code](https://github.com/HazyResearch/safari), [Blog](https://ermongroup.github.io/blog/hyena/), [Tweet](https://twitter.com/dair_ai/status/1634919234835980289?s=20) |
| 8) **OpenICL: An Open-Source Framework for In-context Learning** - a new open-source toolkit for in-context learning and LLM evaluation; supports various state-of-the-art retrieval and inference methods, tasks, and zero-/few-shot evaluation of LLMs.                             | [Paper](https://arxiv.org/abs/2303.02913), [Repo](https://github.com/Shark-NLP/OpenICL), [Tweet](https://twitter.com/dair_ai/status/1634919236954132480?s=20)                                                     |
| 9) **MathPrompter: Mathematical Reasoning using Large Language Models** - a technique that improves LLM performance on mathematical reasoning problems; it uses zero-shot chain-of-thought prompting and verification to ensure generated answers are accurate.                       | [Paper](https://arxiv.org/abs/2303.05398), [Tweet](https://twitter.com/dair_ai/status/1634919239030280197?s=20)                                                                                                   |
| 10) **Scaling up GANs for Text-to-Image Synthesis** - enables scaling up GANs on large datasets for text-to-image synthesis; it’s found to be orders of magnitude faster at inference time, synthesizes high-resolution images, & supports various latent space editing applications. | [Paper](https://arxiv.org/abs/2303.05511), [Project](https://mingukkang.github.io/GigaGAN/) , [Tweet](https://twitter.com/dair_ai/status/1634919241198751744?s=20)                                                |

---

## Feb 27-Mar 5th March 2023 ##

| **Paper**                                                                                                                                                                                                                                                                                                                | **Links**                                                                                                                                                                                                                       |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1) **Language Is Not All You Need: Aligning Perception with Language Models** - introduces a multimodal large language model called Kosmos-1; achieves great performance on language understanding, OCR-free NLP, perception-language tasks, visual QA, and more.                                                        | [Paper](https://arxiv.org/abs/2302.14045), [Tweet](https://twitter.com/dair_ai/status/1632383312550416384?s=20)                                                                                                                 |
| 2) **Evidence of a predictive coding hierarchy in the human brain listening to speech** - finds that human brain activity is best explained by the activations of modern language models enhanced with long-range and hierarchical predictions.                                                                          | [Paper](https://www.nature.com/articles/s41562-022-01516-2?utm_source=twitter&utm_medium=organic_social&utm_campaign=evergreen&utm_content=animation), [Tweet](https://twitter.com/dair_ai/status/1632383315029180416?s=20)     |
| 3) **EvoPrompting: Language Models for Code-Level Neural Architecture Search** - combines evolutionary prompt engineering with soft prompt-tuning to find high-performing models; it leverages few-shot prompting which is further improved by using an evolutionary search approach to improve the in-context examples. | [Paper](https://arxiv.org/abs/2302.14838), [Tweet](https://twitter.com/dair_ai/status/1632383317302562816?s=20)                                                                                                                 |
| 4) **Consistency Models** - a new family of generative models that achieve high sample quality without adversarial training.                                                                                                                                                                                             | [Paper](https://arxiv.org/abs/2303.01469), [Tweet](https://twitter.com/dair_ai/status/1632383319152132096?s=20)                                                                                                                 |
| 5) **Goal Driven Discovery of Distributional Differences via Language Descriptions** - a new task that automatically discovers corpus-level differences via language description in a goal-driven way; applications include discovering insights from commercial reviews and error patterns in NLP systems.              | [Paper](https://arxiv.org/abs/2302.14233) , [Code](https://github.com/ruiqi-zhong/D5), [Tweet](https://twitter.com/dair_ai/status/1632383321035374593?s=20)                                                                     |
| 6) **High-resolution image reconstruction with latent diffusion models from human brain activity** - proposes an approach for high-resolution image reconstruction with latent diffusion models from human brain activity.                                                                                               | [Project](https://sites.google.com/view/stablediffusion-with-brain/) , [Tweet](https://twitter.com/dair_ai/status/1632383323086487554?s=20)                                                                                     |
| 7) **Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control** - a scalable approach to planning with LLMs in embodied settings through grounding functions; GD is found to be a general, flexible, and expressive approach to embodied tasks.                                                 | [Paper](https://grounded-decoding.github.io/paper.pdf), [Project](https://grounded-decoding.github.io/) [Tweet](https://twitter.com/dair_ai/status/1632383325036740610?s=20)                                                    |
| 8) **Language-Driven Representation Learning for Robotics** - a framework for language-driven representation learning from human videos and captions for robotics.                                                                                                                                                       | [Paper](https://arxiv.org/abs/2302.12766), [Models](https://github.com/siddk/voltron-robotics), [Evaluation](https://github.com/siddk/voltron-evaluation), [Tweet](https://twitter.com/dair_ai/status/1632383327154888704?s=20) |
| 9) **Dropout Reduces Underfitting** - demonstrates that dropout can mitigate underfitting when used at the start of training; it counteracts SGD stochasticity and limits the influence of individual batches when training models.                                                                                      | [Paper](https://arxiv.org/abs/2303.01500), [Tweet](https://twitter.com/dair_ai/status/1632383328920666121?s=20)                                                                                                                 |
| 10) **Enabling Conversational Interaction with Mobile UI using Large Language Models** - an approach that enables versatile conversational interactions with mobile UIs using a single LLM.                                                                                                                              | [Paper](https://arxiv.org/abs/2209.08655), [Tweet](https://twitter.com/dair_ai/status/1632383331286253568?s=20)                                                                                                                 |

---

## 20th Feb 20-26th Feb 2023 

| **Paper**                                                                                                                                                                                                                                                                                                | **Links**                                                                                                                                                                                                                   |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1) **LLaMA: Open and Efficient Foundation Language Models** - a 65B parameter foundation model released by Meta AI; relies on publicly available data and outperforms GPT-3 on most benchmarks despite being 10x smaller.                                                                                | [Paper](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/), [Tweet](https://twitter.com/dair_ai/status/1629845535946420226?s=20)                                              |
| 2) **Composer: Creative and Controllable Image Synthesis with Composable Conditions** - a 5B parameter creative and controllable diffusion model trained on billions (text, image) pairs.                                                                                                                | [Paper](https://arxiv.org/abs/2302.09778), [Project](https://damo-vilab.github.io/composer-page/) , [GitHub](https://github.com/damo-vilab/composer) , [Tweet](https://twitter.com/dair_ai/status/1629845537913548802?s=20) |
| 3) **The Wisdom of Hindsight Makes Language Models Better Instruction Followers** - an alternative algorithm to train LLMs from feedback; the feedback is converted to instruction by relabeling the original one and training the model, in a supervised way, for better alignment.                     | [Paper](https://arxiv.org/abs/2302.05206), [GitHub](https://github.com/tianjunz/HIR) [Tweet](https://twitter.com/dair_ai/status/1629845539964481537?s=20)                                                                   |
| 4) **Active Prompting with Chain-of-Thought for Large Language Models** - a prompting technique to adapt LLMs to different task-specific example prompts (annotated with human-designed chain-of-thought reasoning); this process involves finding where the LLM is most uncertain and annotating those. | [Paper](https://arxiv.org/abs/2302.12246), [Code](https://github.com/shizhediao/active-prompt) [Tweet](https://twitter.com/dair_ai/status/1629845541847724033?s=20)                                                         |
| 5) **Modular Deep Learning** - a survey offering a unified view of the building blocks of modular neural networks; it also includes a discussion about modularity in the context of scaling LMs, causal inference, and other key topics in ML.                                                           | [Paper](https://arxiv.org/abs/2302.11529) , [Project](https://www.ruder.io/modular-deep-learning/), [Tweet](https://twitter.com/dair_ai/status/1629845544037228551?s=20)                                                    |
| 6) **Recitation-Augmented Language Models** - an approach that recites passages from the LLM’s own memory to produce final answers; shows high performance on knowledge-intensive tasks.                                                                                                                 | [Paper](https://arxiv.org/abs/2210.01296) , [Tweet](https://twitter.com/dair_ai/status/1629845546276995075?s=20)                                                                                                            |
| 7) **Learning Performance-Improving Code Edits** - an approach that uses LLMs to suggest functionally correct, performance-improving code edits.                                                                                                                                                         | [Paper](https://arxiv.org/abs/2302.07867), [Tweet](https://twitter.com/dair_ai/status/1629845548210561029?s=20)                                                                                                             |
| 8) **More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models** - a comprehensive analysis of novel prompt injection threats to application-integrated LLMs.                                                               | [Paper](https://arxiv.org/abs/2302.12173), [Tweet](https://twitter.com/dair_ai/status/1629845550152523777?s=20)                                                                                                             |
| 9) **Aligning Text-to-Image Models using Human Feedback** - proposes a fine-tuning method to align generative models using human feedback.                                                                                                                                                               | [Paper](https://arxiv.org/abs/2302.12192), [Tweet](https://twitter.com/dair_ai/status/1629845552039968780?s=20)                                                                                                             |
| 10) **MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes** - a memory-efficient radiance field representation for real-time view synthesis of large-scale scenes in a browser.                                                                                      | [Paper](https://arxiv.org/abs/2302.12249), [Tweet](https://twitter.com/dair_ai/status/1629845554061606915?s=20)                                                                                                             |

---

## 13th Feb - 19th Feb 2023 ##

| **Paper**                                                                                                                                                                                                                                                            | **Links**                                                                                                                                                 |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1) **Symbolic Discovery of Optimization Algorithms** - a simple and effective optimization algorithm that’s more memory-efficient than Adam.                                                                                                                         | [Paper](https://arxiv.org/abs/2302.06675), [Tweet](https://twitter.com/dair_ai/status/1627671313874575362?s=20)                                           |
| 2) **Transformer models: an introduction and catalog**                                                                                                                                                                                                               | [Paper](https://arxiv.org/abs/2302.07730), [Tweet](https://twitter.com/dair_ai/status/1627671315678126082?s=20)                                           |
| 3) **3D-aware Conditional Image Synthesis** - a 3D-aware conditional generative model extended with neural radiance fields for controllable photorealistic image synthesis.                                                                                          | [Project](https://www.cs.cmu.edu/~pix2pix3D/) [Tweet](https://twitter.com/dair_ai/status/1627671317355831296?s=20)                                        |
| 4) **The Capacity for Moral Self-Correction in Large Language Models** - finds strong evidence that language models trained with RLHF have the capacity for moral self-correction. The capability emerges at 22B model parameters and typically improves with scale. | [Paper](https://arxiv.org/abs/2302.07459), [Tweet](https://twitter.com/dair_ai/status/1627671319100768260?s=20)                                           |
| 5) **Vision meets RL** - uses reinforcement learning to align computer vision models with task rewards; observes large performance boost across multiple CV tasks such as object detection and colorization.                                                         | [Paper](https://arxiv.org/abs/2302.08242)                                                                                                                 |
| 6) **Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment** - an unsupervised method for text-image alignment that leverages pretrained language models; it enables few-shot image classification with LLMs.                                   | [Paper](https://arxiv.org/abs/2302.00902) , [Code](https://github.com/lhao499/lqae) [Tweet](https://twitter.com/haoliuhl/status/1625273748629901312?s=20) |
| 7) **Augmented Language Models: a Survey** - a survey of language models that are augmented with reasoning skills and the capability to use tools.                                                                                                                   | [Paper](https://arxiv.org/abs/2302.07842), [Tweet](https://twitter.com/dair_ai/status/1627671324477820929?s=20)                                           |
| 8) **Geometric Clifford Algebra Networks** - an approach to incorporate geometry-guided transformations into neural networks using geometric algebra.                                                                                                                | [Paper](https://arxiv.org/abs/2302.06594), [Tweet](https://twitter.com/dair_ai/status/1627671326176473088?s=20)                                           |
| 9) **Auditing large language models: a three-layered approach** - proposes a policy framework for auditing LLMs.                                                                                                                                                     | [Paper](https://arxiv.org/abs/2302.08500), [Tweet](https://twitter.com/dair_ai/status/1627671327950643200?s=20)                                           |
| 10) **Energy Transformer** - a transformer architecture that replaces the sequence of feedforward transformer blocks with a single large Associate Memory model; this follows the popularity that Hopfield Networks have gained in the field of ML.                  | [Paper](https://arxiv.org/abs/2302.07253), [Tweet](https://twitter.com/dair_ai/status/1627671329561346050?s=20)                                           |

---

## 6th Feb 2023 - 12 Feb 2023 ## 

| **Paper**                                                                                                                                                                                                               | **Links**                                                                                                                                                                                           |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1) **Toolformer: Language Models Can Teach Themselves to Use Tools** - introduces language models that teach themselves to use external tools via simple API calls.                                                     | [Paper](https://arxiv.org/abs/2302.04761), [Tweet](https://twitter.com/dair_ai/status/1624832248691191808?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                            |
| 2) **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents** - proposes using language models for open-world game playing.                           | [Paper](https://arxiv.org/abs/2302.01560), [Tweet](https://twitter.com/dair_ai/status/1624832250717036548?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                            |
| 3) **A Categorical Archive of ChatGPT Failures** - a comprehensive analysis of ChatGPT failures for categories like reasoning, factual errors, maths, and coding.                                                       | [Paper](https://arxiv.org/abs/2302.03494), [Tweet](https://twitter.com/dair_ai/status/1624832252587700230?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                            |
| 4) **Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery** - optimizing hard text prompts through efficient gradient-based optimization.                                       | [Paper](https://arxiv.org/abs/2302.03668), [Tweet](https://twitter.com/dair_ai/status/1624832254588465156?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                            |
| 5) **Data Selection for Language Models via Importance Resampling** - proposes a cheap and scalable data selection framework based on an importance resampling algorithm to improve the downstream performance of LMs.  | [Paper](https://arxiv.org/abs/2302.03169), [Tweet](https://twitter.com/dair_ai/status/1624832256400302080?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                            |
| 6) **Structure and Content-Guided Video Synthesis with Diffusion Models** - proposes an approach for structure and content-guided video synthesis with diffusion models.                                                | [Paper](https://arxiv.org/abs/2302.03011) , [Project](https://research.runwayml.com/gen1), [Tweet](https://twitter.com/dair_ai/status/1624832258296229889?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)            |
| 7) **A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity** - performs a more rigorous evaluation of ChatGPt on reasoning, hallucination, and interactivity.      | [Paper](https://arxiv.org/abs/2302.04023), [Tweet](https://twitter.com/dair_ai/status/1624832260213026819?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                            |
| 8) **Noise2Music: Text-conditioned Music Generation with Diffusion Models** - proposes diffusion models to generate high-quality 30-second music clips via text prompts.                                                | [Paper](https://arxiv.org/abs/2302.03917), [Project](https://google-research.github.io/noise2music/), [Tweet](https://twitter.com/dair_ai/status/1624832262163337220?s=20&t=ygX07dsAPDF8_jwrxZIo1Q) |
| 9) **Offsite-Tuning: Transfer Learning without Full Model** - introduces an efficient, privacy-preserving transfer learning framework to adapt foundational models to downstream data without access to the full model. | [Paper](https://arxiv.org/abs/2302.04870), [Project](https://github.com/mit-han-lab/offsite-tuning), [Tweet](https://twitter.com/dair_ai/status/1624832264029831169?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)  |
| 10) **Zero-shot Image-to-Image Translation** - proposes a model for zero-shot image-to-image translation.                                                                                                               | [Paper](https://arxiv.org/abs/2302.03027), [Project](https://pix2pixzero.github.io/), [Tweet](https://twitter.com/dair_ai/status/1624832265967607813?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                 |

---

## 30th Jan 30-5th Feb ##

| **Paper**                                                                                                                                                                                                                                      | **Links**                                                                                                                                                                                     |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1) **REPLUG: Retrieval-Augmented Black-Box Language Models** - a retrieval-augmented LM framework that adapts a retriever to a large-scale, black-box LM like GPT-3.                                                                           | [Paper](https://arxiv.org/abs/2301.12652), [Tweet](https://twitter.com/dair_ai/status/1622261780725616641?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                      |
| 2) **Extracting Training Data from Diffusion Models** - shows that diffusion-based generative models can memorize images from the training data and emit them at generation time.                                                              | [Paper](https://arxiv.org/abs/2301.13188), [Tweet](https://twitter.com/dair_ai/status/1622261782738788353?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                      |
| 3) **The Flan Collection: Designing Data and Methods for Effective Instruction Tuning** - release a more extensive publicly available collection of tasks, templates, and methods to advancing instruction-tuned models.                       | [Paper](https://arxiv.org/abs/2301.13688), [Tweet](https://twitter.com/dair_ai/status/1622261784668241922?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                      |
| 4) **Multimodal Chain-of-Thought Reasoning in Language Models** - incorporates vision features to elicit chain-of-thought reasoning in multimodality, enabling the model to generate effective rationales that contribute to answer inference. | [Paper](https://arxiv.org/abs/2302.00923), [Code](https://github.com/amazon-science/mm-cot) [Tweet](https://twitter.com/dair_ai/status/1622261786559791105?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)     |
| 5) **Dreamix: Video Diffusion Models are General Video Editors** - a diffusion model that performs text-based motion and appearance editing of general videos.                                                                                 | [Paper](https://arxiv.org/abs/2302.01329), [Project](https://dreamix-video-editing.github.io/), [Tweet](https://twitter.com/dair_ai/status/1622261788497657856?s=20&t=ygX07dsAPDF8_jwrxZIo1Q) |
| 6) **Benchmarking Large Language Models for News Summarization**                                                                                                                                                                               | [Paper](https://arxiv.org/abs/2301.13848) , [Tweet](https://twitter.com/dair_ai/status/1622261790326259714?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                     |
| 7) **Mathematical Capabilities of ChatGPT** - investigates the mathematical capabilities of ChatGPT on a new holistic benchmark called GHOSTS.                                                                                                 | [Paper](https://arxiv.org/abs/2301.13867), [Tweet](https://twitter.com/dair_ai/status/1622261792238886913?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                      |
| 8) **Emergence of Maps in the Memories of Blind Navigation Agents** - trains an AI agent to navigate purely by feeling its way around; no use of vision, audio, or any other sensing (as in animals).                                          | [Paper](https://arxiv.org/abs/2301.13261), [Project](https://wijmans.xyz/publication/eom/), [Tweet](https://twitter.com/dair_ai/status/1622261793987989507?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)     |
| 9) **SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections** - a generative model that synthesizes large-scale 3D landscapes from random noises.                                                                               | [Paper](https://arxiv.org/abs/2302.01330), [Tweet](https://twitter.com/dair_ai/status/1622261795925671936?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                      |
| 10) **Large Language Models Can Be Easily Distracted by Irrelevant Context** - finds that many prompting techniques fail when presented with irrelevant context for arithmetic reasoning.                                                      | [Paper](https://arxiv.org/abs/2302.00093), [Tweet](https://twitter.com/dair_ai/status/1622261798379429888?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                      |

---

## 23rd Jan - 29th Jan 2023 ##

| **Paper**                                                                                                                                                                                                                                                            | **Links**                                                                                                                                                                                                                                                                                                            |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1) **MusicLM: Generating Music From Text** - a generative model for generating high-fidelity music from text descriptions.                                                                                                                                           | [Paper](https://arxiv.org/abs/2301.11325), [Tweet](https://twitter.com/dair_ai/status/1619716425761042436?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                                                                                                                                             |
| 2) **Hungry Hungry Hippos: Towards Language Modeling with State Space Models** - an approach to reduce the gap, in terms of performance and hardware utilization, between state space models and attention for language modeling.                                    | [Paper](https://arxiv.org/abs/2212.14052), [Tweet](https://twitter.com/dair_ai/status/1619716427879174144?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                                                                                                                                             |
| 3) **A Watermark for Large Language Models** - a watermarking framework for proprietary language models.                                                                                                                                                             | [Paper](https://arxiv.org/abs/2301.10226), [Tweet](https://twitter.com/dair_ai/status/1619716430127308800?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                                                                                                                                             |
| 4) **Text-To-4D Dynamic Scene Generation** - a new text-to-4D model for dynamic scene generation from input text.                                                                                                                                                    | [Paper](https://arxiv.org/abs/2301.11280), [GitHub](https://make-a-video3d.github.io/), [Tweet](https://twitter.com/dair_ai/status/1619718845018828801?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                                                                                                |
| 5) **ClimaX: A foundation model for weather and climate** - a foundation model for weather and climate, including many capabilities for atmospheric science tasks.                                                                                                   | [Paper](https://arxiv.org/abs/2301.10343), [Tweet](https://twitter.com/tungnd_13/status/1618642574427959296?s=20&t=ygX07dsAPDF8_jwrxZIo1Q), [Blog](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/introducing-climax-the-first-foundation-model-for-weather-and-climate/) |
| 6) **Open Problems in Applied Deep Learning** - If you're looking for interesting open problems in DL, this is a good reference. Not sure if intentional but it also looks useful to get a general picture of current trends in deep learning with \~300 references. | [Paper](https://arxiv.org/abs/2301.11316) , [Tweet](https://twitter.com/dair_ai/status/1619719063915339777?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                                                                                                                                            |
| 7) **DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature** - an approach for zero-shot machine-generated text detection. Uses raw log probabilities from the LLM to determine if the passage was sampled from it.                      | [Paper](https://arxiv.org/abs/2301.11305), [Tweet](https://twitter.com/dair_ai/status/1619719169758613504?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                                                                                                                                             |
| 8) **StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis** - a new model that aims to regain the competitiveness of GANs for fast large-scale text-to-image synthesis.                                                              | [Paper](https://arxiv.org/abs/2301.09515), [Project](https://sites.google.com/view/stylegan-t/), [Code](https://github.com/autonomousvision/stylegan-t) [Tweet](https://twitter.com/dair_ai/status/1619719293779976193?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                                |
| 9) **Large language models generate functional protein sequences across diverse families** - an LLM that can generate protein sequences with a predictable function across large protein families.                                                                   | [Paper](https://www.nature.com/articles/s41587-022-01618-2), [Tweet](https://twitter.com/dair_ai/status/1619719404618645511?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                                                                                                                           |
| 10) **The Impossibility of Parallelizing Boosting** - investigates the possibility of parallelizing boosting.                                                                                                                                                        | [Paper](https://arxiv.org/abs/2301.09627), [Tweet](https://twitter.com/dair_ai/status/1619719511867015168?s=20&t=ygX07dsAPDF8_jwrxZIo1Q)                                                                                                                                                                             |

---

## 16th Jan -22nd Jan ##

| **Paper**                                                                                                                                                                                                                        | **Links**                                                                                                                                                                           |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1) **Google AI Research Recap (2022 Edition)** - an excellent summary of some notable research Google AI did in 2022.                                                                                                            | [Blog](https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html), [Tweet](https://twitter.com/JeffDean/status/1615796030611820545?s=20&t=vUEC8AZmrOJnVxuYIEJs5A) |
| 2) **Dissociating language and thought in large language models: a cognitive perspective** - a review paper on the capabilities of LLMs from a cognitive science perspective.                                                    | [Paper](https://arxiv.org/abs/2301.06627), [Tweet](https://twitter.com/neuranna/status/1615737072207400962?s=20&t=5iWUK4z_rp1NWst7JRbnwg)                                           |
| 3) **Human-Timescale Adaptation in an Open-Ended Task Space** - an agent trained at scale that leads to a general in-content learning algorithm able to adapt to open-ended embodied 3D problems.                                | [Paper](https://arxiv.org/abs/2301.07608), [Tweet](https://twitter.com/FeryalMP/status/1616035293064462338?s=20&t=RN0YZFAXWr-uH2dT2ZTSqQ)                                           |
| 4) **AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation** - an approach to help provide explanations of generative transformer models through memory-efficient attention manipulation. | [Paper](https://arxiv.org/abs/2301.08110), [Tweet](https://twitter.com/JonasAndrulis/status/1616722810608427008?s=20&t=vUEC8AZmrOJnVxuYIEJs5A)                                      |
| 5) **Everything is Connected: Graph Neural Networks** - short overview of key concepts in graph representation learning.                                                                                                         | [Paper](https://arxiv.org/abs/2301.08210), [Tweet](https://twitter.com/PetarV_93/status/1616379369953394688?s=20&t=AqTVY30Y7IZCultzwnqBPA)                                          |
| 6) **GLIGEN: Open-Set Grounded Text-to-Image Generation** - an approach that extends the functionality of existing pre-trained text-to-image diffusion models by enabling conditioning on grounding inputs.                      | [Paper](https://arxiv.org/abs/2301.07093), [Tweet](https://twitter.com/hardmaru/status/1615766551113744384?s=20&t=wx0Y18oSmW0YenXjKRAdnA), [Project](https://gligen.github.io/)     |
| 7) **InstructPix2Pix: Learning to Follow Image Editing Instructions** - proposes a method with the capability of editing images from human instructions.                                                                         | [Paper](https://arxiv.org/abs/2211.09800), [Tweet](https://twitter.com/_akhaliq/status/1615947919286276096?s=20&t=pbRTn8DaPeQFApQ9okkdRg)                                           |
| 8) **Dataset Distillation: A Comprehensive Review**                                                                                                                                                                              | [Paper](https://arxiv.org/abs/2301.07014), [Tweet](https://twitter.com/omarsar0/status/1615745724473540609?s=20&t=r-pwuB6EhbZLXa5R6mL3NQ)                                           |
| 9) **Learning-Rate-Free Learning by D-Adaptation** - a new method for automatically adjusting the learning rate during training, applicable to more than a dozen diverse ML problems.                                            | [Paper](https://arxiv.org/abs/2301.07733), [Tweet](https://twitter.com/aaron_defazio/status/1616453609956478977?s=20&t=hGWDXu4sT5f1KcH-X1IL9g)                                      |
| 10) **RecolorNeRF: Layer Decomposed Radiance Field for Efficient Color Editing of 3D Scenes** - a user-friendly color editing approach for the neural radiance field to achieve a more efficient view-consistent recoloring.     | [Paper](https://arxiv.org/abs/2301.07958), [Tweet](https://twitter.com/_akhaliq/status/1616265465843548160?s=20&t=duiLmtDvxCwkFmw23rYDmQ)                                           |

---

## Jan 9-15 ##

| **Paper**                                                                                                                                                                                                                                                                                                           | **Links**                                                                                                                                                                                  |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1) **Mastering Diverse Domains through World Models** - a general algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in AI.                                                                                                                         | [Paper](https://arxiv.org/abs/2301.04104v1), [Tweet](https://twitter.com/dair_ai/status/1614676677757661185?s=20&t=3GITA7PeX7pGwrqvt97bYQ)                                                 |
| 2) **Tracr: Compiled Transformers as a Laboratory for Interpretability** - a compiler for converting RASP programs into transformer weights. This way of constructing NNs weights enables the development and evaluation of new interpretability tools.                                                             | [Paper](https://arxiv.org/abs/2301.05062), [Tweet](https://twitter.com/dair_ai/status/1614676680165187584?s=20&t=3GITA7PeX7pGwrqvt97bYQ), [Code](https://github.com/deepmind/tracr)        |
| 3) **Multimodal Deep Learning** - multimodal deep learning is a new book published on ArXiv.                                                                                                                                                                                                                        | [Book](https://arxiv.org/abs/2301.04856), [Tweet](https://twitter.com/dair_ai/status/1614676682555670528?s=20&t=3GITA7PeX7pGwrqvt97bYQ)                                                    |
| 4) **Forecasting Potential Misuses of Language Models for Disinformation Campaigns—and How to Reduce Risk** - new work analyzing how generative LMs could potentially be misused for disinformation and how to mitigate these types of risks.                                                                       | [Paper](https://openai.com/blog/forecasting-misuse/), [Tweet](https://twitter.com/dair_ai/status/1614676684984156160?s=20&t=3GITA7PeX7pGwrqvt97bYQ)                                        |
| 5) **Why do Nearest Neighbor Language Models Work?** - empirically identifies reasons why retrieval-augmented LMs (specifically k-nearest neighbor LMs) perform better than standard parametric LMs.                                                                                                                | [Paper](https://arxiv.org/abs/2301.02828), [Code](https://github.com/frankxu2004/knnlm-why), [Tweet](https://twitter.com/dair_ai/status/1614676687597469696?s=20&t=3GITA7PeX7pGwrqvt97bYQ) |
| 6) **Memory Augmented Large Language Models are Computationally Universal** - investigates the use of existing LMs (e.g, Flan-U-PaLM 540B) combined with associative read-write memory to simulate the execution of a universal Turing machine.                                                                     | [Paper](https://arxiv.org/abs/2301.04589) , [Tweet](https://twitter.com/dair_ai/status/1614676689908277252?s=20&t=3GITA7PeX7pGwrqvt97bYQ)                                                  |
| 7) **A Survey on Transformers in Reinforcement Learning** - transformers for RL will be a fascinating research area to track. The same is true for the reverse direction (RL for Transformers)... a notable example: using RLHF to improve LLMs (e.g., ChatGPT).                                                    | [Paper](https://arxiv.org/abs/2301.03044), [Tweet](https://twitter.com/dair_ai/status/1614676692538105860?s=20&t=3GITA7PeX7pGwrqvt97bYQ)                                                   |
| 8) **Scaling Laws for Generative Mixed-Modal Language Models** - introduces scaling laws for generative mixed-modal language models.                                                                                                                                                                                | [Paper](https://arxiv.org/abs/2301.03728), [Tweet](https://twitter.com/dair_ai/status/1614676694920531969?s=20&t=3GITA7PeX7pGwrqvt97bYQ)                                                   |
| 9) **DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching** - a transformer-based network showing robust local feature matching, outperforming the state-of-the-art methods on several benchmarks.                                                                          | [Paper](https://arxiv.org/abs/2301.02993), [Tweet](https://twitter.com/dair_ai/status/1614676697516752898?s=20&t=3GITA7PeX7pGwrqvt97bYQ)                                                   |
| 10) **Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement** - addresses the time series forecasting problem with generative modeling; involves a bidirectional VAE backbone equipped with diffusion, denoising for prediction accuracy, and disentanglement for model interpretability. | [Paper](https://arxiv.org/abs/2301.03028), [Tweet](https://twitter.com/dair_ai/status/1614676699915980804?s=20&t=3GITA7PeX7pGwrqvt97bYQ)                                                   |

---

## Jan 1-8 ## 

| **Paper**                                                                                                                                                                                                                                                                                                         | **Links**                                                                                                                                                                                                                                      |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1) **Muse: Text-To-Image Generation via Masked Generative Transformers** - introduces Muse, a new text-to-image generation model based on masked generative transformers; significantly more efficient than other diffusion models like Imagen and DALLE-2.                                                       | [Paper](https://arxiv.org/abs/2301.00704), [Project](https://muse-model.github.io/), [Code](https://github.com/lucidrains/muse-maskgit-pytorch), [Tweet](https://twitter.com/dair_ai/status/1612153095772938241?s=20&t=ChwZWzSmoRlZKnD54fsV6w) |
| 2) **VALL-E Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers** - introduces VALL-E, a text-to-audio model that performs state-of-the-art zero-shot performance; the text-to-speech synthesis task is treated as a conditional language modeling task.                                       | [Project](https://valle-demo.github.io/), [Tweet](https://twitter.com/dair_ai/status/1612153097962328067?s=20&t=ChwZWzSmoRlZKnD54fsV6w)                                                                                                        |
| 3) **Rethinking with Retrieval: Faithful Large Language Model Inference** - shows the potential of enhancing LLMs by retrieving relevant external knowledge based on decomposed reasoning steps obtained through chain-of-thought prompting.                                                                      | [Paper](https://arxiv.org/abs/2301.00303), [Tweet](https://twitter.com/dair_ai/status/1612153100114055171?s=20&t=ChwZWzSmoRlZKnD54fsV6w)                                                                                                       |
| 4) **SparseGPT: Massive Language Models Can Be Accurately Pruned In One-Shot** - presents a technique for compressing large language models while not sacrificing performance; "pruned to at least 50% sparsity in one-shot, without any retraining."                                                             | [Paper](https://arxiv.org/abs/2301.00774), [Tweet](https://twitter.com/dair_ai/status/1612153102513360901?s=20&t=ChwZWzSmoRlZKnD54fsV6w)                                                                                                       |
| 5) **ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders** - a performant model based on a fully convolutional masked autoencoder framework and other architectural improvements. CNNs are sticking back!                                                                                     | [Paper](https://arxiv.org/abs/2301.00808), [Code](https://github.com/facebookresearch/convnext-v2), [Tweet](https://twitter.com/dair_ai/status/1612153104329281538?s=20&t=ChwZWzSmoRlZKnD54fsV6w)                                              |
| 6) **Large Language Models as Corporate Lobbyists** - with more capabilities, we are starting to see a wider range of applications with LLMs. This paper utilized large language models for conducting corporate lobbying activities.                                                                             | [Paper](https://arxiv.org/abs/2301.01181) , [Code](https://github.com/JohnNay/llm-lobbyist), [Tweet](https://twitter.com/dair_ai/status/1612153106355130372?s=20&t=ChwZWzSmoRlZKnD54fsV6w)                                                     |
| 7) **Superposition, Memorization, and Double Descent** - aims to better understand how deep learning models overfit or memorize examples; interesting phenomena observed; important work toward a mechanistic theory of memorization.                                                                             | [Paper](https://transformer-circuits.pub/2023/toy-double-descent/index.html), [Tweet](https://twitter.com/dair_ai/status/1612153108460892160?s=20&t=ChwZWzSmoRlZKnD54fsV6w)                                                                    |
| 8) **StitchNet: Composing Neural Networks from Pre-Trained Fragments** - new idea to create new coherent neural networks by reusing pretrained fragments of existing NNs. Not straightforward but there is potential in terms of efficiently reusing learned knowledge in pre-trained networks for complex tasks. | [Paper](https://arxiv.org/abs/2301.01947), [Tweet](https://twitter.com/dair_ai/status/1612153110452903936?s=20&t=ChwZWzSmoRlZKnD54fsV6w)                                                                                                       |
| 9) **Iterated Decomposition: Improving Science Q\&A by Supervising Reasoning Processes** - proposes integrated decomposition, an approach to improve Science Q\&A through a human-in-the-loop workflow for refining compositional LM programs.                                                                    | [Paper](https://arxiv.org/abs/2301.01751), [Code](https://github.com/oughtinc/ice) [Tweet](https://twitter.com/dair_ai/status/1612153112638402562?s=20&t=ChwZWzSmoRlZKnD54fsV6w)                                                               |
| 10) **A Succinct Summary of Reinforcement Learning** - a nice overview of some important ideas in RL.                                                                                                                                                                                                             | [Paper](https://arxiv.org/abs/2301.01379), [Tweet](https://twitter.com/dair_ai/status/1612153114773053446?s=20&t=ChwZWzSmoRlZKnD54fsV6w)                                                                                                       |


## AI Models ## 

1. 11th Dec 2023 - Mistral-embed - An embedding model with a 1024 embedding dimension, achieves 55.26 on MTEB - https://mistral.ai/news/mixtral-of-experts/
2. 


## Open AI's Prompt Engineering Handbook ##

https://platform.openai.com/docs/guides/prompt-engineering

## OpenAI Cookbook - Github ## 

https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md#how-to-improve-reliability-on-complex-tasks

## Lilianweng's Gitub Resources & Blogs ##
https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/

## Real-time machine learning: challenges and solutions by @Chip Huyen ## 

https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.htm

## Building LLM applications for production  by @Chip Huyen ## 
https://huyenchip.com/2023/04/11/llm-engineering.html

## Full Stack LLM Bootcamp by @Charles Frye @Sergey Karayev @Josh Tobin ## 
https://fullstackdeeplearning.com/llm-bootcamp/ 

## Prompt Engineering Guide - Excellent Resources for Latest Research Papers on Prompt Engineering by @Elvis Saravia ##
https://www.promptingguide.ai/](https://www.promptingguide.ai/).

## Awesome Resources of AI Machine Learning, MLOPS , Productionizing Machine Learning Models at Scale ##

MadeWithML - @Goku Mohandas
https://madewithml.com/courses/mlops/testing/

Awesome Production Machine Learning - 
https://github.com/EthicalML/awesome-production-machine-learning

Transformers from Scratch - Awesome Transformers Explaination -
https://e2eml.school/transformers.html#resources

Attention is all you need; Attentional Neural Network Models | Łukasz Kaiser | Masterclass -
Vedio Lecture Explaination on Transformers - https://www.youtube.com/watch?v=rBCqOTEfxvg

The Annotated Transformer - Code Implmentation along with Model Architecture -
https://nlp.seas.harvard.edu/2018/04/03/attention.html

## Hugging Face Transformers Library Github ##
https://github.com/huggingface/transformers

### FullStack DeepLearning Prof - @Sergey Karayev - Good Read on GPT Architecture ###
https://dugas.ch/artificial_curiosity/GPT_architecture.html

About - An awesome & curated list of best LLMOps tools for developers -
https://github.com/tensorchord/Awesome-LLMOps

## A list of open LLMs available for commercial use by @eugeneyan ##
https://github.com/eugeneyan/open-llms

@ModularAI - @Mojo 🔥 — a new programming language for all AI developers -
Mojo combines the usability of Python with the performance of C, unlocking unparalleled programmability of AI hardware and extensibility of AI models.

Replit - Training Large Language Models
https://replit.com/@MckayWrigley/Takeoff-School-LangChain-101-Models?v=1

## Lean Copilot: LLMs as Copilots for Theorem Proving in Lean ## 
Lean Copilot allows large language models (LLMs) to be used in Lean for proof automation, e.g., suggesting tactics/premises and searching for proofs. You can use our built-in models from LeanDojo or bring your own models that run either locally (w/ or w/o GPUs) or on the cloud.


## Thank you AI community for visiting my repository - Stay tuned for awesome AI Research Learning Resources ##



